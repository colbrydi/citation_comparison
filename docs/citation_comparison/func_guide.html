<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>citation_comparison.func_guide API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>citation_comparison.func_guide</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import json
import gzip
import os
import re
import pickle
from itertools import compress   # compress(list, boolean)
from collections import Counter
import numpy as np

def read_pwcjson(f_code, f_abs):
    &#34;&#34;&#34; Read two pwc datasets, one for paper&#39;s urls, the other for abstracts.
    Args:
        f_code: the filename for urls
        f_abs: the filename for abstracts
    Returns:
        paper_code: raw urls data
        paper_abs:  raw abstracts data
    &#34;&#34;&#34;
    ## 1) whether the filename are in the dir
    if not os.path.isfile(f_code):
        raise ValueError(&#34;Input urls_text file does not exist: {0}.\
                          I&#39;ll quit now.&#34;.format(f_code))
    if not os.path.isfile(f_abs):
        raise ValueError(&#34;Input abstracts_text file does not exist: {0}.\
                          I&#39;ll quit now.&#34;.format(f_abs))
    ## 2) whether both filenames include &#34;.json&#34;
    if f_code[-5:]  != &#39;.json&#39;:
        raise ValueError(&#34;Not correct json file in urls_text input file.&#34;)
    if f_abs[-5:]  != &#39;.json&#39;:
        raise ValueError(&#34;Not correct json file in abstracts_text input file.&#34;)
    ## load the data
    with open(f_code, &#39;rb&#39;) as file:
        gzip_a = gzip.GzipFile(fileobj = file)         # unzip
        paper_code: list = json.load(gzip_a)    # read json fill
    with open(f_abs, &#39;rb&#39;) as file:
        gzip_a = gzip.GzipFile(fileobj = file)
        paper_abs: list = json.load(gzip_a)
    return (paper_code, paper_abs)

def process_pwc(paper_code, paper_abs):
    &#34;&#34;&#34;
    Process pwc datasts using the following steps.
     1) process &#34;paper_code&#34;: mentioned_in_paper&#34; == True
     2) process &#34;paper_code&#34;: remove duplicates
     3) merge &#34;paper_code&#34; with &#34;paper_abs&#34;
    Args:
        paper_code: raw urls&#39; data
        paper_abs:  raw abstracts&#39; data
    Returns:
        pwc_sub: merged (intersected) papers with url+abs
    &#34;&#34;&#34;
    ### 1) process &#34;paper_code&#34;: mentioned_in_paper&#34; == True
    paper_code = [x for x in paper_code if x[&#39;mentioned_in_paper&#39;] ]
   
    ### 2) process &#34;paper_code&#34;: remove duplicates
    #print( len(paper_code) )
    paper_title = [x[&#39;paper_title&#39;] for x in paper_code]
    #print( len(np.unique(paper_title)) )
    
    title_index = []
    for title, count in Counter(paper_title).items():
        if count &gt; 1:
            title_index.append(title)

    duplicated_items = []
    for title in title_index:
        item_l = []
        for item in paper_code:
            if item[&#39;paper_title&#39;] == title:
                item_l.append(item)
        duplicated_items.append(item_l)
#     print(&#34;The number of unique titles in duplicated_items: &#34;,
#           len(duplicated_items), &#39;\n\t&#39;,
#           sorted(Counter([len(item) for item in duplicated_items]).items(),
#                  key=lambda x:x[0], reverse= True))

    ##Only extracted duplicated items with same github-parent url.
    ##for example, &#39;github/AAA/t1&#39; and &#39;github/AAA/t2&#39;, 
    ##both have same parent &#34;github/AAA&#34;
    duplicated_items_clean = []
    for item in duplicated_items:
        item_repo = [re.search(r&#39;github.com/(.*?)$&#39;, x[&#39;repo_url&#39;]).group(1) for x in item]
        item_repo = [x.split(&#39;/&#39;)[0] if len(x.split(&#39;/&#39;))&gt;0 else x for x in item_repo]
        if len(np.unique(item_repo)) == 1:
            item[0][&#39;repo_url&#39;] = &#39;https://github.com/&#39; + item_repo[0]
            duplicated_items_clean.append(item[0])
#     print(&#34;The number of unique titles in duplicated_items_clean \
#           (not duplicated items here): \n\t&#34;,
#           len(duplicated_items_clean) )

    ##combine &#34;paper_code_unique&#34; and &#34;duplicated_title_clean&#34;
    ##  --&gt; get &#34;paper_code_upd&#34;
    title_unique_index = [title for title, count in Counter(paper_title).items() 
                          if count == 1]
    paper_code_unique = [item for item in paper_code if item[&#39;paper_title&#39;]
                         in title_unique_index]
    print(len(paper_code_unique))
    # generate &#34;paper_code_upd&#34;
    paper_code_upd = paper_code_unique + duplicated_items_clean

    ### 3) merge &#34;paper_code&#34; with &#34;paper_abs&#34;
    urls = [x[&#39;paper_url&#39;] for x in paper_code_upd]
    pwc_sub = [item for item in paper_abs if item[&#39;paper_url&#39;] in urls]
#     print( len(pwc_sub) )
    return pwc_sub

def remove_index_abs(dblp_l):
    &#39;&#39;&#39;Remove &#39;index_abstracts&#39; info&#39;&#39;&#39;
    for prefix, event, value in dblp_l:
        if prefix.startswith(&#39;item.indexed_abstract&#39;):
            continue
        yield prefix, event, value
    
def read_dblpjson(file_link):
    &#34;&#34;&#34; Read large DBLP citation data (skip redundant abstracts info).
    Args:
        file_link: the file link of DBLP data
    Returns:
        dblp: raw dblp data without abstract info
    &#34;&#34;&#34;
    ### 1) read
    dta_dblp = ijson.parse(open(file_link, &#39;r&#39;, encoding=&#34;utf8&#34;))
    ### 2) remove redundant abstracts info by applying func &#34;remove_index_abs()&#34;
    dblp = remove_index_abs(dta_dblp)
    return dblp

def process_dblp(dblp):
    &#34;&#34;&#34; Process dblp data with the following steps.
    1) extract useful attributes, such as &#34;n_citation&#34;..
    Args:
        dblp: raw dblp data without abstract info
    Returns:
        dblp_sub: processed dblp data with selected features
    &#34;&#34;&#34;
    for prefix, event, value in dblp:
        if (prefix, event) == (&#39;item&#39;, &#39;start_map&#39;):
            yield prefix, event, value
    #   record = dict()

    ## extract: id, title, authors_name/id/org, venue_raw/id/type
    ##          year, n_citation,doc_type, publisher, volume, issue, doi,
    ##          fos_name/weight, references_item
    ## note: this raw data does not include &#34;language&#34; or &#34;url&#34;....
        elif (prefix, event) == (&#39;item.id&#39;, &#39;number&#39;):
            yield prefix, event, value
        elif prefix == &#39;item.title&#39;:
            yield prefix, event, value

        elif prefix == &#39;item.authors.item.name&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.authors.item.id&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.authors.item.org&#39;:
            yield prefix, event, value

        elif prefix == &#39;item.venue.raw&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.venue.id&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.venue.type&#39;:
            yield prefix, event, value

        elif prefix == &#39;item.year&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.n_citation&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.doc_type&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.publisher&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.volume&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.issue&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.doi&#39;:
            yield prefix, event, value

        elif prefix == &#39;item.fos.item.name&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.fos.item.w&#39;:
            yield prefix, event, value

        elif prefix == &#39;item.references.item&#39;:
            yield prefix, event, value
#     return dblp_sub
### --&gt; dblp_sub = process_dblp(dblp)

def lowercase_title(dblp_sub):
    &#39;&#39;&#39;Lowercase_title for dblp_sub&#39;&#39;&#39;
    for prefix, element, value in dblp_sub:
        if prefix == &#39;item.title&#39;:
            value = value.lower() if (value[-1] != &#39;.&#39;) else value[:-1].lower()
        yield prefix, element, value
    
def match_title_id(pwc_title, dblp_sub_lt):
    &#39;&#39;&#39;Match titles in pwc_sub with dblp data&#39;&#39;&#39;
    record = dict()
    author_name = []
    author_id = []
    author_org = []

    fos_name = []
    fos_w = []
    ref = []
    # default
    title_match = False

    for prefix, event, value in dblp_sub_lt:
        if (prefix, event) == (&#39;item&#39;, &#39;start_map&#39;):
            ## return results only if &#34;title_match&#34; == True
            if title_match:
                record[&#39;authors_name&#39;] = author_name
                record[&#39;authors_id&#39;] = author_id
                record[&#39;authors_org&#39;] = author_org
                record[&#39;fos_name&#39;] = fos_name
                record[&#39;fos_w&#39;] = fos_w
                record[&#39;ref&#39;] = ref  # references_item
                yield record

            record = dict()
            author_name = []
            author_id = []
            author_org = []

            fos_name = []
            fos_w = []
            ref = []  # references_item

        elif prefix == &#39;item.id&#39;:
            record[&#39;id&#39;] = value
    
        elif prefix == &#39;item.title&#39;:
            ## filter title in pwc_title
            if value in pwc_title:
                title_match = True
                record[&#39;title&#39;] = value
            else:
                title_match = False
                continue   # --&gt; use &#34;continue&#34; not &#34;break&#34; !!
            # end of filter/match title

        elif prefix == &#39;item.authors.item.name&#39;:
            author_name.append(value)
        elif prefix == &#39;item.authors.item.id&#39;:
            author_id.append(value)
        elif prefix == &#39;item.authors.item.org&#39;:
            author_org.append(value )
      
        elif prefix == &#39;item.venue.raw&#39;:
            record[&#39;venue_raw&#39;] = value
        elif prefix == &#39;item.venue.id&#39;:
            record[&#39;venue_id&#39;] = value
        elif prefix == &#39;item.venue.type&#39;:
            record[&#39;venue_type&#39;] = value
      
        elif prefix == &#39;item.year&#39;:
            record[&#39;year&#39;] = value
        elif prefix == &#39;item.n_citation&#39;:
            record[&#39;n_citation&#39;] = value
        elif prefix == &#39;item.doc_type&#39;:
            record[&#39;doc_type&#39;] = value
        elif prefix == &#39;item.publisher&#39;:
            record[&#39;publisher&#39;] = value
        elif prefix == &#39;item.volume&#39;:
            record[&#39;volume&#39;] = value
        elif prefix == &#39;item.issue&#39;:
            record[&#39;issue&#39;] = value
        elif prefix == &#39;item.doi&#39;:
            record[&#39;doi&#39;] = value

        elif prefix == &#39;item.fos.item.name&#39;:
            fos_name.append(value)
        elif prefix == &#39;item.fos.item.w&#39;:
            fos_w.append(value)

        elif prefix == &#39;item.references.item&#39;:
            ref.append(value)

def merge_pwc_dblp(pwc_sub, dblp_sub):
    &#34;&#34;&#34; Left join pwc_sub with dblp_sub using common (&#34;title&#34; &amp; &#34;year&#34;).
    Args:
        pwc_sub: processed dataset 1 (papers with url+abs)
        dblp_sub: processed dataset 2 (dblp_with selected features)
    Returns:
        pwc_dblp_trt: cleaned treatment data (with urls)
    &#34;&#34;&#34;
    ## extract &#34;pwc_title&#34;: &lt;-- lower case
    pwc_title = list( map(lambda x: x[&#39;paper_title&#39;].lower()
                          if (x[&#39;paper_title&#39;][-1] != &#39;.&#39;)
                          else x[&#39;paper_title&#39;][:-1].lower(),
                          pwc_sub) )
    ## lowercase_title for dblp_sub -- &#34;lowercase_title()&#34; func
    dblp_sub_lt = lowercase_title(dblp_sub)
    
    ## match --- &#34;match_title_id()&#34; func
    cite_match = match_title_id(pwc_title, dblp_sub_lt)
    dblp_match_cs = list(cite_match)
    
    ## lowercase title for &#34;paper_code_abs_upd&#34;
    for item in pwc_sub:
        if item[&#39;paper_title&#39;][-1] != &#39;.&#39;:
            item[&#39;paper_title&#39;] = item[&#39;paper_title&#39;].lower()
        else:
            item[&#39;paper_title&#39;] = item[&#39;paper_title&#39;][:-1].lower()

    ## match -- get &#34;pwc_dblp_trt&#34;
    pwc_dblp_trt = []
    for code in pwc_sub:  # code: pwc data feature
        for cite in dblp_match_cs:   # cite: citation data feature
            if code[&#39;paper_title&#39;] == cite[&#39;title&#39;]:
                if int(code[&#39;date&#39;][:4]) == cite[&#39;year&#39;]:
                    cite_sub = { key:value for key,value in cite.items()
                                if key in [&#39;id&#39;, &#39;n_citation&#39;, &#39;doc_type&#39;, &#39;publisher&#39;,
                                           &#39;volume&#39;,&#39;issue&#39;,&#39;doi&#39;, &#39;venue_raw&#39;,
                                           &#39;venue_id&#39;,&#39;venue_type&#39;, &#39;authors_name&#39;, 
                                           &#39;authors_id&#39;,&#39;authors_org&#39;,
                                           &#39;fos_name&#39;, &#39;fos_w&#39;, &#39;ref&#39;]}
                    pwc_dblp_trt.append( {**code , **cite_sub} )
                    break
    
    ## clean &#34;venue_raw&#34;
    for item in pwc_dblp_trt:
        if &#39;venue_raw&#39; in item.keys():
            if item[&#39;venue_raw&#39;][:7] == &#39;arXiv: &#39;:
                item[&#39;venue_raw&#39;] = item[&#39;venue_raw&#39;][7:]
    ### save &#34;pwc_dblp_trt&#34; !!!!
#     with open(&#39;pwc_dblp_trt.pkl&#39;, &#39;wb&#39;) as out:
#         pickle.dump(pwc_dblp_trt, out)
    return pwc_dblp_trt


def gen_control(paper_code, paper_abs, dblp):
    &#34;&#34;&#34;Generate control group.
    Args:
        paper_code: raw urls&#39; data
        paper_abs:  raw abstracts&#39; data
        dblp: raw dblp data without abstract info
    Returns:
        pwc_dblp_con: cleaned controled data (without urls)
    &#34;&#34;&#34;
    #return pwc_dblp_con

def matching(pwc_dblp_trt, pwc_dblp_con):
    &#34;&#34;&#34; For each treated item in &#34;pwc_dblp_trt&#34;.
    select a matched item as control
    Args:
        pwc_dblp_trt: cleaned treatment data (with urls)
        pwc_dblp_con: cleaned controled data (without urls)
    Returns:
        matched_dta: the dataset with treated and control items.
    &#34;&#34;&#34;
    #return matched_dta
    </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="citation_comparison.func_guide.gen_control"><code class="name flex">
<span>def <span class="ident">gen_control</span></span>(<span>paper_code, paper_abs, dblp)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate control group.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>paper_code</code></strong></dt>
<dd>raw urls' data</dd>
<dt><strong><code>paper_abs</code></strong></dt>
<dd>raw abstracts' data</dd>
<dt><strong><code>dblp</code></strong></dt>
<dd>raw dblp data without abstract info</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pwc_dblp_con</code></dt>
<dd>cleaned controled data (without urls)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gen_control(paper_code, paper_abs, dblp):
    &#34;&#34;&#34;Generate control group.
    Args:
        paper_code: raw urls&#39; data
        paper_abs:  raw abstracts&#39; data
        dblp: raw dblp data without abstract info
    Returns:
        pwc_dblp_con: cleaned controled data (without urls)
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="citation_comparison.func_guide.lowercase_title"><code class="name flex">
<span>def <span class="ident">lowercase_title</span></span>(<span>dblp_sub)</span>
</code></dt>
<dd>
<div class="desc"><p>Lowercase_title for dblp_sub</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lowercase_title(dblp_sub):
    &#39;&#39;&#39;Lowercase_title for dblp_sub&#39;&#39;&#39;
    for prefix, element, value in dblp_sub:
        if prefix == &#39;item.title&#39;:
            value = value.lower() if (value[-1] != &#39;.&#39;) else value[:-1].lower()
        yield prefix, element, value</code></pre>
</details>
</dd>
<dt id="citation_comparison.func_guide.match_title_id"><code class="name flex">
<span>def <span class="ident">match_title_id</span></span>(<span>pwc_title, dblp_sub_lt)</span>
</code></dt>
<dd>
<div class="desc"><p>Match titles in pwc_sub with dblp data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def match_title_id(pwc_title, dblp_sub_lt):
    &#39;&#39;&#39;Match titles in pwc_sub with dblp data&#39;&#39;&#39;
    record = dict()
    author_name = []
    author_id = []
    author_org = []

    fos_name = []
    fos_w = []
    ref = []
    # default
    title_match = False

    for prefix, event, value in dblp_sub_lt:
        if (prefix, event) == (&#39;item&#39;, &#39;start_map&#39;):
            ## return results only if &#34;title_match&#34; == True
            if title_match:
                record[&#39;authors_name&#39;] = author_name
                record[&#39;authors_id&#39;] = author_id
                record[&#39;authors_org&#39;] = author_org
                record[&#39;fos_name&#39;] = fos_name
                record[&#39;fos_w&#39;] = fos_w
                record[&#39;ref&#39;] = ref  # references_item
                yield record

            record = dict()
            author_name = []
            author_id = []
            author_org = []

            fos_name = []
            fos_w = []
            ref = []  # references_item

        elif prefix == &#39;item.id&#39;:
            record[&#39;id&#39;] = value
    
        elif prefix == &#39;item.title&#39;:
            ## filter title in pwc_title
            if value in pwc_title:
                title_match = True
                record[&#39;title&#39;] = value
            else:
                title_match = False
                continue   # --&gt; use &#34;continue&#34; not &#34;break&#34; !!
            # end of filter/match title

        elif prefix == &#39;item.authors.item.name&#39;:
            author_name.append(value)
        elif prefix == &#39;item.authors.item.id&#39;:
            author_id.append(value)
        elif prefix == &#39;item.authors.item.org&#39;:
            author_org.append(value )
      
        elif prefix == &#39;item.venue.raw&#39;:
            record[&#39;venue_raw&#39;] = value
        elif prefix == &#39;item.venue.id&#39;:
            record[&#39;venue_id&#39;] = value
        elif prefix == &#39;item.venue.type&#39;:
            record[&#39;venue_type&#39;] = value
      
        elif prefix == &#39;item.year&#39;:
            record[&#39;year&#39;] = value
        elif prefix == &#39;item.n_citation&#39;:
            record[&#39;n_citation&#39;] = value
        elif prefix == &#39;item.doc_type&#39;:
            record[&#39;doc_type&#39;] = value
        elif prefix == &#39;item.publisher&#39;:
            record[&#39;publisher&#39;] = value
        elif prefix == &#39;item.volume&#39;:
            record[&#39;volume&#39;] = value
        elif prefix == &#39;item.issue&#39;:
            record[&#39;issue&#39;] = value
        elif prefix == &#39;item.doi&#39;:
            record[&#39;doi&#39;] = value

        elif prefix == &#39;item.fos.item.name&#39;:
            fos_name.append(value)
        elif prefix == &#39;item.fos.item.w&#39;:
            fos_w.append(value)

        elif prefix == &#39;item.references.item&#39;:
            ref.append(value)</code></pre>
</details>
</dd>
<dt id="citation_comparison.func_guide.matching"><code class="name flex">
<span>def <span class="ident">matching</span></span>(<span>pwc_dblp_trt, pwc_dblp_con)</span>
</code></dt>
<dd>
<div class="desc"><p>For each treated item in "pwc_dblp_trt".
select a matched item as control</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pwc_dblp_trt</code></strong></dt>
<dd>cleaned treatment data (with urls)</dd>
<dt><strong><code>pwc_dblp_con</code></strong></dt>
<dd>cleaned controled data (without urls)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>matched_dta</code></dt>
<dd>the dataset with treated and control items.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def matching(pwc_dblp_trt, pwc_dblp_con):
    &#34;&#34;&#34; For each treated item in &#34;pwc_dblp_trt&#34;.
    select a matched item as control
    Args:
        pwc_dblp_trt: cleaned treatment data (with urls)
        pwc_dblp_con: cleaned controled data (without urls)
    Returns:
        matched_dta: the dataset with treated and control items.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="citation_comparison.func_guide.merge_pwc_dblp"><code class="name flex">
<span>def <span class="ident">merge_pwc_dblp</span></span>(<span>pwc_sub, dblp_sub)</span>
</code></dt>
<dd>
<div class="desc"><p>Left join pwc_sub with dblp_sub using common ("title" &amp; "year").</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pwc_sub</code></strong></dt>
<dd>processed dataset 1 (papers with url+abs)</dd>
<dt><strong><code>dblp_sub</code></strong></dt>
<dd>processed dataset 2 (dblp_with selected features)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pwc_dblp_trt</code></dt>
<dd>cleaned treatment data (with urls)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_pwc_dblp(pwc_sub, dblp_sub):
    &#34;&#34;&#34; Left join pwc_sub with dblp_sub using common (&#34;title&#34; &amp; &#34;year&#34;).
    Args:
        pwc_sub: processed dataset 1 (papers with url+abs)
        dblp_sub: processed dataset 2 (dblp_with selected features)
    Returns:
        pwc_dblp_trt: cleaned treatment data (with urls)
    &#34;&#34;&#34;
    ## extract &#34;pwc_title&#34;: &lt;-- lower case
    pwc_title = list( map(lambda x: x[&#39;paper_title&#39;].lower()
                          if (x[&#39;paper_title&#39;][-1] != &#39;.&#39;)
                          else x[&#39;paper_title&#39;][:-1].lower(),
                          pwc_sub) )
    ## lowercase_title for dblp_sub -- &#34;lowercase_title()&#34; func
    dblp_sub_lt = lowercase_title(dblp_sub)
    
    ## match --- &#34;match_title_id()&#34; func
    cite_match = match_title_id(pwc_title, dblp_sub_lt)
    dblp_match_cs = list(cite_match)
    
    ## lowercase title for &#34;paper_code_abs_upd&#34;
    for item in pwc_sub:
        if item[&#39;paper_title&#39;][-1] != &#39;.&#39;:
            item[&#39;paper_title&#39;] = item[&#39;paper_title&#39;].lower()
        else:
            item[&#39;paper_title&#39;] = item[&#39;paper_title&#39;][:-1].lower()

    ## match -- get &#34;pwc_dblp_trt&#34;
    pwc_dblp_trt = []
    for code in pwc_sub:  # code: pwc data feature
        for cite in dblp_match_cs:   # cite: citation data feature
            if code[&#39;paper_title&#39;] == cite[&#39;title&#39;]:
                if int(code[&#39;date&#39;][:4]) == cite[&#39;year&#39;]:
                    cite_sub = { key:value for key,value in cite.items()
                                if key in [&#39;id&#39;, &#39;n_citation&#39;, &#39;doc_type&#39;, &#39;publisher&#39;,
                                           &#39;volume&#39;,&#39;issue&#39;,&#39;doi&#39;, &#39;venue_raw&#39;,
                                           &#39;venue_id&#39;,&#39;venue_type&#39;, &#39;authors_name&#39;, 
                                           &#39;authors_id&#39;,&#39;authors_org&#39;,
                                           &#39;fos_name&#39;, &#39;fos_w&#39;, &#39;ref&#39;]}
                    pwc_dblp_trt.append( {**code , **cite_sub} )
                    break
    
    ## clean &#34;venue_raw&#34;
    for item in pwc_dblp_trt:
        if &#39;venue_raw&#39; in item.keys():
            if item[&#39;venue_raw&#39;][:7] == &#39;arXiv: &#39;:
                item[&#39;venue_raw&#39;] = item[&#39;venue_raw&#39;][7:]
    ### save &#34;pwc_dblp_trt&#34; !!!!
#     with open(&#39;pwc_dblp_trt.pkl&#39;, &#39;wb&#39;) as out:
#         pickle.dump(pwc_dblp_trt, out)
    return pwc_dblp_trt</code></pre>
</details>
</dd>
<dt id="citation_comparison.func_guide.process_dblp"><code class="name flex">
<span>def <span class="ident">process_dblp</span></span>(<span>dblp)</span>
</code></dt>
<dd>
<div class="desc"><p>Process dblp data with the following steps.
1) extract useful attributes, such as "n_citation"..</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dblp</code></strong></dt>
<dd>raw dblp data without abstract info</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dblp_sub</code></dt>
<dd>processed dblp data with selected features</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_dblp(dblp):
    &#34;&#34;&#34; Process dblp data with the following steps.
    1) extract useful attributes, such as &#34;n_citation&#34;..
    Args:
        dblp: raw dblp data without abstract info
    Returns:
        dblp_sub: processed dblp data with selected features
    &#34;&#34;&#34;
    for prefix, event, value in dblp:
        if (prefix, event) == (&#39;item&#39;, &#39;start_map&#39;):
            yield prefix, event, value
    #   record = dict()

    ## extract: id, title, authors_name/id/org, venue_raw/id/type
    ##          year, n_citation,doc_type, publisher, volume, issue, doi,
    ##          fos_name/weight, references_item
    ## note: this raw data does not include &#34;language&#34; or &#34;url&#34;....
        elif (prefix, event) == (&#39;item.id&#39;, &#39;number&#39;):
            yield prefix, event, value
        elif prefix == &#39;item.title&#39;:
            yield prefix, event, value

        elif prefix == &#39;item.authors.item.name&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.authors.item.id&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.authors.item.org&#39;:
            yield prefix, event, value

        elif prefix == &#39;item.venue.raw&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.venue.id&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.venue.type&#39;:
            yield prefix, event, value

        elif prefix == &#39;item.year&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.n_citation&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.doc_type&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.publisher&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.volume&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.issue&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.doi&#39;:
            yield prefix, event, value

        elif prefix == &#39;item.fos.item.name&#39;:
            yield prefix, event, value
        elif prefix == &#39;item.fos.item.w&#39;:
            yield prefix, event, value

        elif prefix == &#39;item.references.item&#39;:
            yield prefix, event, value</code></pre>
</details>
</dd>
<dt id="citation_comparison.func_guide.process_pwc"><code class="name flex">
<span>def <span class="ident">process_pwc</span></span>(<span>paper_code, paper_abs)</span>
</code></dt>
<dd>
<div class="desc"><p>Process pwc datasts using the following steps.
1) process "paper_code": mentioned_in_paper" == True
2) process "paper_code": remove duplicates
3) merge "paper_code" with "paper_abs"</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>paper_code</code></strong></dt>
<dd>raw urls' data</dd>
<dt><strong><code>paper_abs</code></strong></dt>
<dd>raw abstracts' data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pwc_sub</code></dt>
<dd>merged (intersected) papers with url+abs</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_pwc(paper_code, paper_abs):
    &#34;&#34;&#34;
    Process pwc datasts using the following steps.
     1) process &#34;paper_code&#34;: mentioned_in_paper&#34; == True
     2) process &#34;paper_code&#34;: remove duplicates
     3) merge &#34;paper_code&#34; with &#34;paper_abs&#34;
    Args:
        paper_code: raw urls&#39; data
        paper_abs:  raw abstracts&#39; data
    Returns:
        pwc_sub: merged (intersected) papers with url+abs
    &#34;&#34;&#34;
    ### 1) process &#34;paper_code&#34;: mentioned_in_paper&#34; == True
    paper_code = [x for x in paper_code if x[&#39;mentioned_in_paper&#39;] ]
   
    ### 2) process &#34;paper_code&#34;: remove duplicates
    #print( len(paper_code) )
    paper_title = [x[&#39;paper_title&#39;] for x in paper_code]
    #print( len(np.unique(paper_title)) )
    
    title_index = []
    for title, count in Counter(paper_title).items():
        if count &gt; 1:
            title_index.append(title)

    duplicated_items = []
    for title in title_index:
        item_l = []
        for item in paper_code:
            if item[&#39;paper_title&#39;] == title:
                item_l.append(item)
        duplicated_items.append(item_l)
#     print(&#34;The number of unique titles in duplicated_items: &#34;,
#           len(duplicated_items), &#39;\n\t&#39;,
#           sorted(Counter([len(item) for item in duplicated_items]).items(),
#                  key=lambda x:x[0], reverse= True))

    ##Only extracted duplicated items with same github-parent url.
    ##for example, &#39;github/AAA/t1&#39; and &#39;github/AAA/t2&#39;, 
    ##both have same parent &#34;github/AAA&#34;
    duplicated_items_clean = []
    for item in duplicated_items:
        item_repo = [re.search(r&#39;github.com/(.*?)$&#39;, x[&#39;repo_url&#39;]).group(1) for x in item]
        item_repo = [x.split(&#39;/&#39;)[0] if len(x.split(&#39;/&#39;))&gt;0 else x for x in item_repo]
        if len(np.unique(item_repo)) == 1:
            item[0][&#39;repo_url&#39;] = &#39;https://github.com/&#39; + item_repo[0]
            duplicated_items_clean.append(item[0])
#     print(&#34;The number of unique titles in duplicated_items_clean \
#           (not duplicated items here): \n\t&#34;,
#           len(duplicated_items_clean) )

    ##combine &#34;paper_code_unique&#34; and &#34;duplicated_title_clean&#34;
    ##  --&gt; get &#34;paper_code_upd&#34;
    title_unique_index = [title for title, count in Counter(paper_title).items() 
                          if count == 1]
    paper_code_unique = [item for item in paper_code if item[&#39;paper_title&#39;]
                         in title_unique_index]
    print(len(paper_code_unique))
    # generate &#34;paper_code_upd&#34;
    paper_code_upd = paper_code_unique + duplicated_items_clean

    ### 3) merge &#34;paper_code&#34; with &#34;paper_abs&#34;
    urls = [x[&#39;paper_url&#39;] for x in paper_code_upd]
    pwc_sub = [item for item in paper_abs if item[&#39;paper_url&#39;] in urls]
#     print( len(pwc_sub) )
    return pwc_sub</code></pre>
</details>
</dd>
<dt id="citation_comparison.func_guide.read_dblpjson"><code class="name flex">
<span>def <span class="ident">read_dblpjson</span></span>(<span>file_link)</span>
</code></dt>
<dd>
<div class="desc"><p>Read large DBLP citation data (skip redundant abstracts info).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_link</code></strong></dt>
<dd>the file link of DBLP data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dblp</code></dt>
<dd>raw dblp data without abstract info</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_dblpjson(file_link):
    &#34;&#34;&#34; Read large DBLP citation data (skip redundant abstracts info).
    Args:
        file_link: the file link of DBLP data
    Returns:
        dblp: raw dblp data without abstract info
    &#34;&#34;&#34;
    ### 1) read
    dta_dblp = ijson.parse(open(file_link, &#39;r&#39;, encoding=&#34;utf8&#34;))
    ### 2) remove redundant abstracts info by applying func &#34;remove_index_abs()&#34;
    dblp = remove_index_abs(dta_dblp)
    return dblp</code></pre>
</details>
</dd>
<dt id="citation_comparison.func_guide.read_pwcjson"><code class="name flex">
<span>def <span class="ident">read_pwcjson</span></span>(<span>f_code, f_abs)</span>
</code></dt>
<dd>
<div class="desc"><p>Read two pwc datasets, one for paper's urls, the other for abstracts.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f_code</code></strong></dt>
<dd>the filename for urls</dd>
<dt><strong><code>f_abs</code></strong></dt>
<dd>the filename for abstracts</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>paper_code</code></dt>
<dd>raw urls data</dd>
<dt><code>paper_abs</code></dt>
<dd>raw abstracts data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_pwcjson(f_code, f_abs):
    &#34;&#34;&#34; Read two pwc datasets, one for paper&#39;s urls, the other for abstracts.
    Args:
        f_code: the filename for urls
        f_abs: the filename for abstracts
    Returns:
        paper_code: raw urls data
        paper_abs:  raw abstracts data
    &#34;&#34;&#34;
    ## 1) whether the filename are in the dir
    if not os.path.isfile(f_code):
        raise ValueError(&#34;Input urls_text file does not exist: {0}.\
                          I&#39;ll quit now.&#34;.format(f_code))
    if not os.path.isfile(f_abs):
        raise ValueError(&#34;Input abstracts_text file does not exist: {0}.\
                          I&#39;ll quit now.&#34;.format(f_abs))
    ## 2) whether both filenames include &#34;.json&#34;
    if f_code[-5:]  != &#39;.json&#39;:
        raise ValueError(&#34;Not correct json file in urls_text input file.&#34;)
    if f_abs[-5:]  != &#39;.json&#39;:
        raise ValueError(&#34;Not correct json file in abstracts_text input file.&#34;)
    ## load the data
    with open(f_code, &#39;rb&#39;) as file:
        gzip_a = gzip.GzipFile(fileobj = file)         # unzip
        paper_code: list = json.load(gzip_a)    # read json fill
    with open(f_abs, &#39;rb&#39;) as file:
        gzip_a = gzip.GzipFile(fileobj = file)
        paper_abs: list = json.load(gzip_a)
    return (paper_code, paper_abs)</code></pre>
</details>
</dd>
<dt id="citation_comparison.func_guide.remove_index_abs"><code class="name flex">
<span>def <span class="ident">remove_index_abs</span></span>(<span>dblp_l)</span>
</code></dt>
<dd>
<div class="desc"><p>Remove 'index_abstracts' info</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_index_abs(dblp_l):
    &#39;&#39;&#39;Remove &#39;index_abstracts&#39; info&#39;&#39;&#39;
    for prefix, event, value in dblp_l:
        if prefix.startswith(&#39;item.indexed_abstract&#39;):
            continue
        yield prefix, event, value</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="citation_comparison" href="index.html">citation_comparison</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="citation_comparison.func_guide.gen_control" href="#citation_comparison.func_guide.gen_control">gen_control</a></code></li>
<li><code><a title="citation_comparison.func_guide.lowercase_title" href="#citation_comparison.func_guide.lowercase_title">lowercase_title</a></code></li>
<li><code><a title="citation_comparison.func_guide.match_title_id" href="#citation_comparison.func_guide.match_title_id">match_title_id</a></code></li>
<li><code><a title="citation_comparison.func_guide.matching" href="#citation_comparison.func_guide.matching">matching</a></code></li>
<li><code><a title="citation_comparison.func_guide.merge_pwc_dblp" href="#citation_comparison.func_guide.merge_pwc_dblp">merge_pwc_dblp</a></code></li>
<li><code><a title="citation_comparison.func_guide.process_dblp" href="#citation_comparison.func_guide.process_dblp">process_dblp</a></code></li>
<li><code><a title="citation_comparison.func_guide.process_pwc" href="#citation_comparison.func_guide.process_pwc">process_pwc</a></code></li>
<li><code><a title="citation_comparison.func_guide.read_dblpjson" href="#citation_comparison.func_guide.read_dblpjson">read_dblpjson</a></code></li>
<li><code><a title="citation_comparison.func_guide.read_pwcjson" href="#citation_comparison.func_guide.read_pwcjson">read_pwcjson</a></code></li>
<li><code><a title="citation_comparison.func_guide.remove_index_abs" href="#citation_comparison.func_guide.remove_index_abs">remove_index_abs</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>