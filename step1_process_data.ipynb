{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import citation_comparison.step1_func_guide as FG\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. read and process PWC data #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DOS: Add more descriptions**\n",
    "\n",
    "1) introduce two PWC datasets (urls, abstracts);\n",
    "\n",
    "2) how to process_pwc? and get trt and con groups\n",
    "\n",
    "3) basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWC_code data: 37480 \n",
      "PWC_abs data: 133824\n",
      "{'paper_url': 'https://paperswithcode.com/paper/unsupervised-training-for-3d-morphable-model', 'paper_title': 'Unsupervised Training for 3D Morphable Model Regression', 'paper_arxiv_id': '1806.06098', 'paper_url_abs': 'http://arxiv.org/abs/1806.06098v1', 'paper_url_pdf': 'http://arxiv.org/pdf/1806.06098v1.pdf', 'repo_url': 'https://github.com/google/tf_mesh_renderer', 'mentioned_in_paper': True, 'mentioned_in_github': False, 'framework': 'tf'} \n",
      "\n",
      " {'paper_url': 'https://paperswithcode.com/paper/multi-hop-knowledge-graph-reasoning-with', 'arxiv_id': '1808.10568', 'title': 'Multi-Hop Knowledge Graph Reasoning with Reward Shaping', 'abstract': 'Multi-hop reasoning is an effective approach for query answering (QA) over\\nincomplete knowledge graphs (KGs). The problem can be formulated in a\\nreinforcement learning (RL) setup, where a policy-based agent sequentially\\nextends its inference path until it reaches a target. However, in an incomplete\\nKG environment, the agent receives low-quality rewards corrupted by false\\nnegatives in the training data, which harms generalization at test time.\\nFurthermore, since no golden action sequence is used for training, the agent\\ncan be misled by spurious search trajectories that incidentally lead to the\\ncorrect answer. We propose two modeling advances to address both issues: (1) we\\nreduce the impact of false negative supervision by adopting a pretrained\\none-hop embedding model to estimate the reward of unobserved facts; (2) we\\ncounter the sensitivity to spurious paths of on-policy RL by forcing the agent\\nto explore a diverse set of paths using randomly generated edge masks. Our\\napproach significantly improves over existing path-based KGQA models on several\\nbenchmark datasets and is comparable or better than embedding-based models.', 'url_abs': 'http://arxiv.org/abs/1808.10568v2', 'url_pdf': 'http://arxiv.org/pdf/1808.10568v2.pdf', 'proceeding': 'EMNLP 2018 10', 'authors': ['Xi Victoria Lin', 'Richard Socher', 'Caiming Xiong'], 'tasks': ['Knowledge Graphs'], 'date': '2018-08-31'}\n"
     ]
    }
   ],
   "source": [
    "### 1.1) introduce two PWC datasets (urls, abstracts);\n",
    "## \n",
    "f_code = \"./Test_data/urls.json\"\n",
    "f_abs =  \"./Test_data/abstracts.json\"\n",
    "##\n",
    "\n",
    "paper_code, paper_abs = FG.read_pwcjson(f_code, f_abs)\n",
    "print(\"PWC_code data: {} \\nPWC_abs data: {}\".format(len(paper_code), len(paper_abs) ) )\n",
    "## an example:\n",
    "print( paper_code[0], '\\n\\n', paper_abs[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWC_treatment samples: 10835 \n",
      "PWC_control samples: 7818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'paper_url': 'https://paperswithcode.com/paper/large-scale-plant-classification-with-deep',\n",
       " 'arxiv_id': '1706.03736',\n",
       " 'title': 'Large-Scale Plant Classification with Deep Neural Networks',\n",
       " 'abstract': 'This paper discusses the potential of applying deep learning techniques for\\nplant classification and its usage for citizen science in large-scale\\nbiodiversity monitoring. We show that plant classification using near\\nstate-of-the-art convolutional network architectures like ResNet50 achieves\\nsignificant improvements in accuracy compared to the most widespread plant\\nclassification application in test sets composed of thousands of different\\nspecies labels. We find that the predictions can be confidently used as a\\nbaseline classification in citizen science communities like iNaturalist (or its\\nSpanish fork, Natusfera) which in turn can share their data with biodiversity\\nportals like GBIF.',\n",
       " 'url_abs': 'http://arxiv.org/abs/1706.03736v1',\n",
       " 'url_pdf': 'http://arxiv.org/pdf/1706.03736v1.pdf',\n",
       " 'proceeding': None,\n",
       " 'authors': ['Ignacio Heredia'],\n",
       " 'tasks': [],\n",
       " 'date': '2017-06-12'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 1.2) how to process_pwc? and get trt and con groups\n",
    "\n",
    "## \"pwc_sub_trt/con\": merged (intersected) papers with url+abs\n",
    "##          -- Treatment group!!\n",
    "pwc_sub_trt = FG.process_pwc(paper_code, paper_abs, mention_paper=True)\n",
    "pwc_sub_con = FG.process_pwc(paper_code, paper_abs, mention_paper=False)\n",
    "\n",
    "print( \"PWC_treatment samples: {} \\nPWC_control samples: {}\".format(len(pwc_sub_trt), len(pwc_sub_con) ) ) \n",
    "\n",
    "## an example with basic features\n",
    "pwc_sub_trt[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3) basic features**\n",
    "\n",
    "The above example is one paper record from the pwc data, which includes the following features:\n",
    "- paper_url:\n",
    "- arxiv_id:\n",
    "- title:\n",
    "- abstract:\n",
    "- url_abs, url_pdf\n",
    "- proceeding\n",
    "- authors\n",
    "- tasks\n",
    "- date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save \"pwc_sub_trt\" and \"pwc_sub_con\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Test_data/src_data/pwc_sub_trt.data', 'wb') as f:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(pwc_sub_trt, f)\n",
    "    \n",
    "with open('./Test_data/src_data/pwc_sub_con.data', 'wb') as f:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(pwc_sub_con, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ \"pwc_sub_trt\" and \"pwc_sub_con\"\n",
    "# with open('./Test_data/src_data/pwc_sub_trt.data', 'rb') as f:\n",
    "#     # read the data as binary data stream\n",
    "#     pwc_sub_trt = pickle.load(f)\n",
    "\n",
    "# with open('./Test_data/src_data/pwc_sub_con.data', 'rb') as f:\n",
    "#     # read the data as binary data stream\n",
    "#     pwc_sub_con = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. read and process DBLP data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DOS: Add more descriptions**\n",
    "\n",
    "1) introduce *LARGE JSON* DBLP datasets;\n",
    "\n",
    "2) how to process_dblp with JSON FORMAT? then extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download LARGE DBLP DATA (12GB), GET \"dblp.v12.json\"\n",
    "\n",
    "# !pip install wget\n",
    "# !wget https://originalstatic.aminer.cn/misc/dblp.v12.7z  \n",
    "# !p7zip -d 'dblp.v12.7z'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Test_data/dblp.v12.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a2bde30141b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./Test_data/dblp.v12.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdta_dblp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mijson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdblp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_index_abs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdta_dblp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Test_data/dblp.v12.json'"
     ]
    }
   ],
   "source": [
    "### 2.1) introduce LARGE DBLP datasets;\n",
    "\n",
    "# !pip install ijson\n",
    "import ijson\n",
    "\n",
    "file = \"./Test_data/dblp.v12.json\"\n",
    "dta_dblp = ijson.parse(open(file, 'r', encoding=\"utf8\"))\n",
    "dblp = FG.remove_index_abs(dta_dblp)\n",
    "\n",
    "## an example\n",
    "i = 0\n",
    "for prefix, event, value in dblp:\n",
    "    if i < 25:\n",
    "        print(prefix, event, value)\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will Process JSON data and Extract the following features from the JSON format, and finally convert to DATA FRAME\n",
    "- id, title, authors_name/id/org, venue_raw/id/type\n",
    "- year, n_citation,doc_type, publisher, volume, issue, doi,\n",
    "- fos_name/weight, references_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2) how to process_dblp with JSON FORMAT? Then extract features\n",
    "\n",
    "## \"dblp_sub\": processed dblp data with selected features\n",
    "dblp_sub = FG.process_dblp(dblp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Merge PWC and DBLP using \"merge_pwc_dblp\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe more details --> **TO DOS**\n",
    "\n",
    "For BOTH \"treatment\" and \"control\" pwc_sub_trt/con data: \n",
    "- Do the following steps!\n",
    "\n",
    "**Note: the following codes in Part 3 \"TOOK VERY LONG TIME\" to RUN**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1) For TREATMENT group: \"pwc_sub_trt\"** --> return **\"match_trt\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For TREATMENT group: \"pwc_sub_trt\" ###\n",
    "pwc_sub_file = pwc_sub_trt\n",
    "##\n",
    "\n",
    "## a) extract \"pwc_title\": <-- lower case\n",
    "pwc_title = list( map(lambda x: x['title'].lower()\n",
    "                      if (x['title'][-1] != '.')\n",
    "                      else x['title'][:-1].lower(),\n",
    "                      pwc_sub_file) )\n",
    "\n",
    "## b) lowercase_title for dblp_sub -- \"lowercase_title()\" func\n",
    "dblp_sub_lt = FG.lowercase_title(dblp_sub)\n",
    "\n",
    "## c) match title--- \"match_title_id()\" func\n",
    "cite_match = FG.match_title_id(pwc_title, dblp_sub_lt)\n",
    "dblp_match_cs = list(cite_match)\n",
    "\n",
    "print( len(pwc_title), len(dblp_match_cs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## d) match and clean \n",
    "\n",
    "## lowercase title for \"pwc_sub_file\"\n",
    "for item in pwc_sub_file:\n",
    "    if item['title'][-1] != '.':\n",
    "        item['title'] = item['title'].lower()\n",
    "    else:\n",
    "        item['title'] = item['title'][:-1].lower()\n",
    "\n",
    "\n",
    "## match -- get \"out\", then redefine as \"match_trt\"\n",
    "out = []\n",
    "for code in pwc_sub_file:  # code: pwc data feature\n",
    "    for cite in dblp_match_cs:   # cite: citation data feature\n",
    "        if code['title'] == cite['title']:\n",
    "            if int(code['date'][:4]) == cite['year']:\n",
    "                cite_sub = { key:value for key,value in cite.items()\n",
    "                            if key in ['id', 'n_citation', 'doc_type', 'publisher',\n",
    "                                       'volume','issue','doi', 'venue_raw',\n",
    "                                       'venue_id','venue_type', 'authors_name', \n",
    "                                       'authors_id','authors_org',\n",
    "                                       'fos_name', 'fos_w', 'ref']}\n",
    "                out.append( {**code , **cite_sub} )\n",
    "                break\n",
    "\n",
    "## clean \"venue_raw\"\n",
    "for item in out:\n",
    "    if 'venue_raw' in item.keys():\n",
    "        if item['venue_raw'][:7] == 'arXiv: ':\n",
    "            item['venue_raw'] = item['venue_raw'][7:]\n",
    "\n",
    "#################\n",
    "## redefine \"out\" as \"match_trt\"\n",
    "match_trt = out.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### conver list to DataFrame\n",
    "match_trt_df = pd.DataFrame(match_trt)\n",
    "print('Matched treatment groups: ', match_trt_df.shape)\n",
    "match_trt_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save \"match_trt_df\"\n",
    "match_trt_df.to_csv('./Test_data/src_data/match_trt_df.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2) For CONTROL GROUP: \"pwc_sub_con\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Here, we RERUN 2.1 and 2.2 for \"dblp_sub\" \n",
    "### 2.1) \n",
    "file = \"./Test_data/dblp.v12.json\"\n",
    "dta_dblp = ijson.parse(open(file, 'r', encoding=\"utf8\"))\n",
    "dblp = FG.remove_index_abs(dta_dblp)\n",
    "\n",
    "### 2.2) how to process_dblp with JSON FORMAT? Then extract features\n",
    "dblp_sub = FG.process_dblp(dblp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For CONTROL GROUP: \"pwc_sub_con\" ###\n",
    "pwc_sub_file = pwc_sub_con\n",
    "##\n",
    "\n",
    "## a) extract \"pwc_title\": <-- lower case\n",
    "pwc_title = list( map(lambda x: x['title'].lower()\n",
    "                      if (x['title'][-1] != '.')\n",
    "                      else x['title'][:-1].lower(),\n",
    "                      pwc_sub_file) )\n",
    "\n",
    "## b) lowercase_title for dblp_sub -- \"lowercase_title()\" func\n",
    "dblp_sub_lt = FG.lowercase_title(dblp_sub)\n",
    "\n",
    "# ## c) match title--- \"match_title_id()\" func\n",
    "cite_match = FG.match_title_id(pwc_title, dblp_sub_lt)\n",
    "dblp_match_cs = list(cite_match)\n",
    "\n",
    "# print( len(pwc_title), len(dblp_match_cs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## d) match and clean \n",
    "\n",
    "## lowercase title for \"paper_code_abs_upd\"\n",
    "for item in pwc_sub_file:\n",
    "    if item['title'][-1] != '.':\n",
    "        item['title'] = item['title'].lower()\n",
    "    else:\n",
    "        item['title'] = item['title'][:-1].lower()\n",
    "\n",
    "\n",
    "## match -- get \"out\", then redefine as \"match_trt\"\n",
    "out = []\n",
    "for code in pwc_sub_file:  # code: pwc data feature\n",
    "    for cite in dblp_match_cs:   # cite: citation data feature\n",
    "        if code['title'] == cite['title']:\n",
    "            if int(code['date'][:4]) == cite['year']:\n",
    "                cite_sub = { key:value for key,value in cite.items()\n",
    "                            if key in ['id', 'n_citation', 'doc_type', 'publisher',\n",
    "                                       'volume','issue','doi', 'venue_raw',\n",
    "                                       'venue_id','venue_type', 'authors_name', \n",
    "                                       'authors_id','authors_org',\n",
    "                                       'fos_name', 'fos_w', 'ref']}\n",
    "                out.append( {**code , **cite_sub} )\n",
    "                break\n",
    "\n",
    "## clean \"venue_raw\"\n",
    "for item in out:\n",
    "    if 'venue_raw' in item.keys():\n",
    "        if item['venue_raw'][:7] == 'arXiv: ':\n",
    "            item['venue_raw'] = item['venue_raw'][7:]\n",
    "\n",
    "#################\n",
    "## redefine \"out\"\n",
    "match_con = out\n",
    "\n",
    "print('Matched Controled groups: ', len(match_con))\n",
    "match_con[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### conver list to DataFrame\n",
    "match_con_df = pd.DataFrame(match_con)\n",
    "print('Matched controlled groups: ', match_con_df.shape)\n",
    "match_con_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save \"match_con_df\"\n",
    "match_con_df.to_csv('./Test_data/src_data/match_con_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
