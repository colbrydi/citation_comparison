{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ijson in c:\\users\\chenj107\\appdata\\roaming\\python\\python37\\site-packages (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "import step1_func_guide as FG\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. read and process PWC data #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DOS: Add more descriptions**\n",
    "\n",
    "1) introduce two PWC datasets (urls, abstracts);\n",
    "\n",
    "2) how to process_pwc? and get trt and con groups\n",
    "\n",
    "3) basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWC_code data: 37480 \n",
      "PWC_abs data: 133824\n",
      "{'paper_url': 'https://paperswithcode.com/paper/unsupervised-training-for-3d-morphable-model', 'paper_title': 'Unsupervised Training for 3D Morphable Model Regression', 'paper_arxiv_id': '1806.06098', 'paper_url_abs': 'http://arxiv.org/abs/1806.06098v1', 'paper_url_pdf': 'http://arxiv.org/pdf/1806.06098v1.pdf', 'repo_url': 'https://github.com/google/tf_mesh_renderer', 'mentioned_in_paper': True, 'mentioned_in_github': False, 'framework': 'tf'} \n",
      "\n",
      " {'paper_url': 'https://paperswithcode.com/paper/multi-hop-knowledge-graph-reasoning-with', 'arxiv_id': '1808.10568', 'title': 'Multi-Hop Knowledge Graph Reasoning with Reward Shaping', 'abstract': 'Multi-hop reasoning is an effective approach for query answering (QA) over\\nincomplete knowledge graphs (KGs). The problem can be formulated in a\\nreinforcement learning (RL) setup, where a policy-based agent sequentially\\nextends its inference path until it reaches a target. However, in an incomplete\\nKG environment, the agent receives low-quality rewards corrupted by false\\nnegatives in the training data, which harms generalization at test time.\\nFurthermore, since no golden action sequence is used for training, the agent\\ncan be misled by spurious search trajectories that incidentally lead to the\\ncorrect answer. We propose two modeling advances to address both issues: (1) we\\nreduce the impact of false negative supervision by adopting a pretrained\\none-hop embedding model to estimate the reward of unobserved facts; (2) we\\ncounter the sensitivity to spurious paths of on-policy RL by forcing the agent\\nto explore a diverse set of paths using randomly generated edge masks. Our\\napproach significantly improves over existing path-based KGQA models on several\\nbenchmark datasets and is comparable or better than embedding-based models.', 'url_abs': 'http://arxiv.org/abs/1808.10568v2', 'url_pdf': 'http://arxiv.org/pdf/1808.10568v2.pdf', 'proceeding': 'EMNLP 2018 10', 'authors': ['Xi Victoria Lin', 'Richard Socher', 'Caiming Xiong'], 'tasks': ['Knowledge Graphs'], 'date': '2018-08-31'}\n"
     ]
    }
   ],
   "source": [
    "### 1.1) introduce two PWC datasets (urls, abstracts);\n",
    "## \n",
    "f_code = \"../Test_data/urls.json\"\n",
    "f_abs =  \"../Test_data/abstracts.json\"\n",
    "##\n",
    "\n",
    "paper_code, paper_abs = FG.read_pwcjson(f_code, f_abs)\n",
    "print(\"PWC_code data: {} \\nPWC_abs data: {}\".format(len(paper_code), len(paper_abs) ) )\n",
    "## an example:\n",
    "print( paper_code[0], '\\n\\n', paper_abs[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWC_treatment samples: 10835 \n",
      "PWC_control samples: 7818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'paper_url': 'https://paperswithcode.com/paper/large-scale-plant-classification-with-deep',\n",
       " 'arxiv_id': '1706.03736',\n",
       " 'title': 'Large-Scale Plant Classification with Deep Neural Networks',\n",
       " 'abstract': 'This paper discusses the potential of applying deep learning techniques for\\nplant classification and its usage for citizen science in large-scale\\nbiodiversity monitoring. We show that plant classification using near\\nstate-of-the-art convolutional network architectures like ResNet50 achieves\\nsignificant improvements in accuracy compared to the most widespread plant\\nclassification application in test sets composed of thousands of different\\nspecies labels. We find that the predictions can be confidently used as a\\nbaseline classification in citizen science communities like iNaturalist (or its\\nSpanish fork, Natusfera) which in turn can share their data with biodiversity\\nportals like GBIF.',\n",
       " 'url_abs': 'http://arxiv.org/abs/1706.03736v1',\n",
       " 'url_pdf': 'http://arxiv.org/pdf/1706.03736v1.pdf',\n",
       " 'proceeding': None,\n",
       " 'authors': ['Ignacio Heredia'],\n",
       " 'tasks': [],\n",
       " 'date': '2017-06-12'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 1.2) how to process_pwc? and get trt and con groups\n",
    "\n",
    "## \"pwc_sub_trt/con\": merged (intersected) papers with url+abs\n",
    "##          -- Treatment group!!\n",
    "pwc_sub_trt = FG.process_pwc(paper_code, paper_abs, mention_paper=True)\n",
    "pwc_sub_con = FG.process_pwc(paper_code, paper_abs, mention_paper=False)\n",
    "\n",
    "print( \"PWC_treatment samples: {} \\nPWC_control samples: {}\".format(len(pwc_sub_trt), len(pwc_sub_con) ) ) \n",
    "\n",
    "## an example with basic features\n",
    "pwc_sub_trt[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3) basic features**\n",
    "\n",
    "The above example is one paper record from the pwc data, which includes the following features:\n",
    "- paper_url:\n",
    "- arxiv_id:\n",
    "- title:\n",
    "- abstract:\n",
    "- url_abs, url_pdf\n",
    "- proceeding\n",
    "- authors\n",
    "- tasks\n",
    "- date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save \"pwc_sub_trt\" and \"pwc_sub_con\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Test_data/src_data/pwc_sub_trt.data', 'wb') as f:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(pwc_sub_trt, f)\n",
    "    \n",
    "with open('../Test_data/src_data/pwc_sub_con.data', 'wb') as f:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(pwc_sub_con, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## READ \"pwc_sub_trt\" and \"pwc_sub_con\"\n",
    "# with open('../Test_data/src_data/pwc_sub_trt.data', 'rb') as f:\n",
    "#     # read the data as binary data stream\n",
    "#     pwc_sub_trt = pickle.load(f)\n",
    "\n",
    "# with open('../Test_data/src_data/pwc_sub_con.data', 'rb') as f:\n",
    "#     # read the data as binary data stream\n",
    "#     pwc_sub_con = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. read and process DBLP data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DOS: Add more descriptions**\n",
    "\n",
    "1) introduce *LARGE JSON* DBLP datasets;\n",
    "\n",
    "2) how to process_dblp with JSON FORMAT? then extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download LARGE DBLP DATA (12GB), GET \"dblp.v12.json\"\n",
    "\n",
    "# !pip install wget\n",
    "# !wget https://originalstatic.aminer.cn/misc/dblp.v12.7z  \n",
    "# !p7zip -d 'dblp.v12.7z'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " start_array None\n",
      "item start_map None\n",
      "item map_key id\n",
      "item.id number 1091\n",
      "item map_key authors\n",
      "item.authors start_array None\n",
      "item.authors.item start_map None\n",
      "item.authors.item map_key name\n",
      "item.authors.item.name string Makoto Satoh\n",
      "item.authors.item map_key org\n",
      "item.authors.item.org string Shinshu University\n",
      "item.authors.item map_key id\n",
      "item.authors.item.id number 2312688602\n",
      "item.authors.item end_map None\n",
      "item.authors.item start_map None\n",
      "item.authors.item map_key name\n",
      "item.authors.item.name string Ryo Muramatsu\n",
      "item.authors.item map_key org\n",
      "item.authors.item.org string Shinshu University\n",
      "item.authors.item map_key id\n",
      "item.authors.item.id number 2482909946\n",
      "item.authors.item end_map None\n",
      "item.authors.item start_map None\n",
      "item.authors.item map_key name\n",
      "item.authors.item.name string Mizue Kayama\n"
     ]
    }
   ],
   "source": [
    "### 2.1) introduce LARGE DBLP datasets;\n",
    "\n",
    "# !pip install ijson\n",
    "import ijson\n",
    "\n",
    "file = \"../Test_data/dblp.v12.json\"\n",
    "dta_dblp = ijson.parse(open(file, 'r', encoding=\"utf8\"))\n",
    "dblp = FG.remove_index_abs(dta_dblp)\n",
    "\n",
    "## an example\n",
    "i = 0\n",
    "for prefix, event, value in dblp:\n",
    "    if i < 25:\n",
    "        print(prefix, event, value)\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will Process JSON data and Extract the following features from the JSON format, and finally convert to DATA FRAME\n",
    "- id, title, authors_name/id/org, venue_raw/id/type\n",
    "- year, n_citation,doc_type, publisher, volume, issue, doi,\n",
    "- fos_name/weight, references_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2) how to process_dblp with JSON FORMAT? Then extract features\n",
    "\n",
    "## \"dblp_sub\": processed dblp data with selected features\n",
    "dblp_sub = FG.process_dblp(dblp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Merge PWC and DBLP using \"merge_pwc_dblp\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe more details --> **TO DOS**\n",
    "\n",
    "For BOTH \"treatment\" and \"control\" pwc_sub_trt/con data: \n",
    "- Do the following steps!\n",
    "\n",
    "**Note: the following codes in Part 3 TOOK VERY LONG TIME to RUN**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1) For TREATMENT group: \"pwc_sub_trt\"** --> return **\"match_trt\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For TREATMENT group: \"pwc_sub_trt\" ###\n",
    "pwc_sub_file = pwc_sub_trt\n",
    "##\n",
    "\n",
    "## a) extract \"pwc_title\": <-- lower case\n",
    "pwc_title = list( map(lambda x: x['title'].lower()\n",
    "                      if (x['title'][-1] != '.')\n",
    "                      else x['title'][:-1].lower(),\n",
    "                      pwc_sub_file) )\n",
    "\n",
    "## b) lowercase_title for dblp_sub -- \"lowercase_title()\" func\n",
    "dblp_sub_lt = FG.lowercase_title(dblp_sub)\n",
    "\n",
    "## c) match title--- \"match_title_id()\" func\n",
    "cite_match = FG.match_title_id(pwc_title, dblp_sub_lt)\n",
    "dblp_match_cs = list(cite_match)\n",
    "\n",
    "print( len(pwc_title), len(dblp_match_cs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## d) match and clean \n",
    "\n",
    "## lowercase title for \"pwc_sub_file\"\n",
    "for item in pwc_sub_file:\n",
    "    if item['title'][-1] != '.':\n",
    "        item['title'] = item['title'].lower()\n",
    "    else:\n",
    "        item['title'] = item['title'][:-1].lower()\n",
    "\n",
    "\n",
    "## match -- get \"out\", then redefine as \"match_trt\"\n",
    "out = []\n",
    "for code in pwc_sub_file:  # code: pwc data feature\n",
    "    for cite in dblp_match_cs:   # cite: citation data feature\n",
    "        if code['title'] == cite['title']:\n",
    "            if int(code['date'][:4]) == cite['year']:\n",
    "                cite_sub = { key:value for key,value in cite.items()\n",
    "                            if key in ['id', 'n_citation', 'doc_type', 'publisher',\n",
    "                                       'volume','issue','doi', 'venue_raw',\n",
    "                                       'venue_id','venue_type', 'authors_name', \n",
    "                                       'authors_id','authors_org',\n",
    "                                       'fos_name', 'fos_w', 'ref']}\n",
    "                out.append( {**code , **cite_sub} )\n",
    "                break\n",
    "\n",
    "## clean \"venue_raw\"\n",
    "for item in out:\n",
    "    if 'venue_raw' in item.keys():\n",
    "        if item['venue_raw'][:7] == 'arXiv: ':\n",
    "            item['venue_raw'] = item['venue_raw'][7:]\n",
    "\n",
    "#################\n",
    "## redefine \"out\" as \"match_trt\"\n",
    "match_trt = out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched treatment groups:  (6826, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_url</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url_abs</th>\n",
       "      <th>url_pdf</th>\n",
       "      <th>proceeding</th>\n",
       "      <th>authors</th>\n",
       "      <th>tasks</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>doi</th>\n",
       "      <th>venue_raw</th>\n",
       "      <th>venue_id</th>\n",
       "      <th>venue_type</th>\n",
       "      <th>authors_name</th>\n",
       "      <th>authors_id</th>\n",
       "      <th>authors_org</th>\n",
       "      <th>fos_name</th>\n",
       "      <th>fos_w</th>\n",
       "      <th>ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>https://paperswithcode.com/paper/large-scale-p...</td>\n",
       "      <td>1706.03736</td>\n",
       "      <td>large-scale plant classification with deep neu...</td>\n",
       "      <td>This paper discusses the potential of applying...</td>\n",
       "      <td>http://arxiv.org/abs/1706.03736v1</td>\n",
       "      <td>http://arxiv.org/pdf/1706.03736v1.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Ignacio Heredia]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>...</td>\n",
       "      <td>10.1145/3075564.3075590</td>\n",
       "      <td>Computing Frontiers</td>\n",
       "      <td>2.626327e+09</td>\n",
       "      <td>C</td>\n",
       "      <td>[Ignacio Heredia]</td>\n",
       "      <td>[2622430314]</td>\n",
       "      <td>[Instituto de Fisica de Cantabria (CSIC-UC), A...</td>\n",
       "      <td>[Fork (system call), Data mining, Computer sci...</td>\n",
       "      <td>[0.4434, 0.45259, 0.43328, 0.41814, 0.45741, 0...</td>\n",
       "      <td>[1522301498, 1556850077, 1606347560, 191666523...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           paper_url    arxiv_id  \\\n",
       "0  https://paperswithcode.com/paper/large-scale-p...  1706.03736   \n",
       "\n",
       "                                               title  \\\n",
       "0  large-scale plant classification with deep neu...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  This paper discusses the potential of applying...   \n",
       "\n",
       "                             url_abs                                url_pdf  \\\n",
       "0  http://arxiv.org/abs/1706.03736v1  http://arxiv.org/pdf/1706.03736v1.pdf   \n",
       "\n",
       "  proceeding            authors tasks        date  ...  \\\n",
       "0       None  [Ignacio Heredia]    []  2017-06-12  ...   \n",
       "\n",
       "                       doi            venue_raw      venue_id venue_type  \\\n",
       "0  10.1145/3075564.3075590  Computing Frontiers  2.626327e+09          C   \n",
       "\n",
       "        authors_name    authors_id  \\\n",
       "0  [Ignacio Heredia]  [2622430314]   \n",
       "\n",
       "                                         authors_org  \\\n",
       "0  [Instituto de Fisica de Cantabria (CSIC-UC), A...   \n",
       "\n",
       "                                            fos_name  \\\n",
       "0  [Fork (system call), Data mining, Computer sci...   \n",
       "\n",
       "                                               fos_w  \\\n",
       "0  [0.4434, 0.45259, 0.43328, 0.41814, 0.45741, 0...   \n",
       "\n",
       "                                                 ref  \n",
       "0  [1522301498, 1556850077, 1606347560, 191666523...  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### conver list to DataFrame\n",
    "match_trt_df = pd.DataFrame(match_trt)\n",
    "print('Matched treatment groups: ', match_trt_df.shape)\n",
    "match_trt_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save \"match_trt_df\"\n",
    "match_trt_df.to_csv('../Test_data/src_data/match_trt_df.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2) For CONTROL GROUP: \"pwc_sub_con\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Here, we RERUN 2.1 and 2.2 for \"dblp_sub\" \n",
    "### 2.1) \n",
    "file = \"../Test_data/dblp.v12.json\"\n",
    "dta_dblp = ijson.parse(open(file, 'r', encoding=\"utf8\"))\n",
    "dblp = FG.remove_index_abs(dta_dblp)\n",
    "\n",
    "### 2.2) how to process_dblp with JSON FORMAT? Then extract features\n",
    "dblp_sub = FG.process_dblp(dblp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For CONTROL GROUP: \"pwc_sub_con\" ###\n",
    "pwc_sub_file = pwc_sub_con\n",
    "##\n",
    "\n",
    "## a) extract \"pwc_title\": <-- lower case\n",
    "pwc_title = list( map(lambda x: x['title'].lower()\n",
    "                      if (x['title'][-1] != '.')\n",
    "                      else x['title'][:-1].lower(),\n",
    "                      pwc_sub_file) )\n",
    "\n",
    "## b) lowercase_title for dblp_sub -- \"lowercase_title()\" func\n",
    "dblp_sub_lt = FG.lowercase_title(dblp_sub)\n",
    "\n",
    "# ## c) match title--- \"match_title_id()\" func\n",
    "cite_match = FG.match_title_id(pwc_title, dblp_sub_lt)\n",
    "dblp_match_cs = list(cite_match)\n",
    "\n",
    "# print( len(pwc_title), len(dblp_match_cs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Controled groups:  5132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'paper_url': 'https://paperswithcode.com/paper/building-an-interpretable-recommender-via',\n",
       " 'arxiv_id': '1606.05819',\n",
       " 'title': 'building an interpretable recommender via loss-preserving transformation',\n",
       " 'abstract': 'We propose a method for building an interpretable recommender system for\\npersonalizing online content and promotions. Historical data available for the\\nsystem consists of customer features, provided content (promotions), and user\\nresponses. Unlike in a standard multi-class classification setting,\\nmisclassification costs depend on both recommended actions and customers. Our\\nmethod transforms such a data set to a new set which can be used with standard\\ninterpretable multi-class classification algorithms. The transformation has the\\ndesirable property that minimizing the standard misclassification penalty in\\nthis new space is equivalent to minimizing the custom cost function.',\n",
       " 'url_abs': 'http://arxiv.org/abs/1606.05819v1',\n",
       " 'url_pdf': 'http://arxiv.org/pdf/1606.05819v1.pdf',\n",
       " 'proceeding': None,\n",
       " 'authors': ['Amit Dhurandhar', 'Sechan Oh', 'Marek Petrik'],\n",
       " 'tasks': ['Recommendation Systems'],\n",
       " 'date': '2016-06-19',\n",
       " 'id': 2462769205,\n",
       " 'n_citation': 3,\n",
       " 'doc_type': 'Repository',\n",
       " 'publisher': '',\n",
       " 'volume': '',\n",
       " 'issue': '',\n",
       " 'doi': '',\n",
       " 'venue_raw': 'Machine Learning',\n",
       " 'venue_id': 2597365278,\n",
       " 'venue_type': 'J',\n",
       " 'authors_name': ['Amit Dhurandhar', 'Sechan Oh', 'Marek Petrik'],\n",
       " 'authors_id': [2061123877, 2627091534, 1979722320],\n",
       " 'authors_org': [],\n",
       " 'fos_name': ['Recommender system',\n",
       "  'Data mining',\n",
       "  'Computer science',\n",
       "  'Artificial intelligence',\n",
       "  'Statistical classification',\n",
       "  'Machine learning'],\n",
       " 'fos_w': [Decimal('0.56516'),\n",
       "  Decimal('0.48009'),\n",
       "  Decimal('0.47038'),\n",
       "  Decimal('0.0'),\n",
       "  Decimal('0.54189'),\n",
       "  Decimal('0.48324')],\n",
       " 'ref': [72187777,\n",
       "  134253518,\n",
       "  2005530589,\n",
       "  2038030840,\n",
       "  2098576843,\n",
       "  2138909795,\n",
       "  2142261479,\n",
       "  2187558068,\n",
       "  2243760329]}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## d) match and clean \n",
    "\n",
    "## lowercase title for \"paper_code_abs_upd\"\n",
    "for item in pwc_sub_file:\n",
    "    if item['title'][-1] != '.':\n",
    "        item['title'] = item['title'].lower()\n",
    "    else:\n",
    "        item['title'] = item['title'][:-1].lower()\n",
    "\n",
    "\n",
    "## match -- get \"out\", then redefine as \"match_trt\"\n",
    "out = []\n",
    "for code in pwc_sub_file:  # code: pwc data feature\n",
    "    for cite in dblp_match_cs:   # cite: citation data feature\n",
    "        if code['title'] == cite['title']:\n",
    "            if int(code['date'][:4]) == cite['year']:\n",
    "                cite_sub = { key:value for key,value in cite.items()\n",
    "                            if key in ['id', 'n_citation', 'doc_type', 'publisher',\n",
    "                                       'volume','issue','doi', 'venue_raw',\n",
    "                                       'venue_id','venue_type', 'authors_name', \n",
    "                                       'authors_id','authors_org',\n",
    "                                       'fos_name', 'fos_w', 'ref']}\n",
    "                out.append( {**code , **cite_sub} )\n",
    "                break\n",
    "\n",
    "## clean \"venue_raw\"\n",
    "for item in out:\n",
    "    if 'venue_raw' in item.keys():\n",
    "        if item['venue_raw'][:7] == 'arXiv: ':\n",
    "            item['venue_raw'] = item['venue_raw'][7:]\n",
    "\n",
    "#################\n",
    "## redefine \"out\"\n",
    "match_con = out\n",
    "\n",
    "print('Matched Controled groups: ', len(match_con))\n",
    "match_con[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched controlled groups:  (5132, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_url</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url_abs</th>\n",
       "      <th>url_pdf</th>\n",
       "      <th>proceeding</th>\n",
       "      <th>authors</th>\n",
       "      <th>tasks</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>doi</th>\n",
       "      <th>venue_raw</th>\n",
       "      <th>venue_id</th>\n",
       "      <th>venue_type</th>\n",
       "      <th>authors_name</th>\n",
       "      <th>authors_id</th>\n",
       "      <th>authors_org</th>\n",
       "      <th>fos_name</th>\n",
       "      <th>fos_w</th>\n",
       "      <th>ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>https://paperswithcode.com/paper/deep-predicti...</td>\n",
       "      <td>1805.07526</td>\n",
       "      <td>deep predictive coding network with local recu...</td>\n",
       "      <td>Inspired by \"predictive coding\" - a theory in ...</td>\n",
       "      <td>http://arxiv.org/abs/1805.07526v2</td>\n",
       "      <td>http://arxiv.org/pdf/1805.07526v2.pdf</td>\n",
       "      <td>NeurIPS 2018 12</td>\n",
       "      <td>[Kuan Han, Haiguang Wen, Yizhen Zhang, Di Fu, ...</td>\n",
       "      <td>[Image Classification, Object Recognition]</td>\n",
       "      <td>2018-05-19</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>Computer Vision and Pattern Recognition</td>\n",
       "      <td>2.597176e+09</td>\n",
       "      <td>J</td>\n",
       "      <td>[Kuan Han, Haiguang Wen, Yizhen Zhang, Di Fu, ...</td>\n",
       "      <td>[2749467312, 2211170770, 2555490721, 280503237...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Pattern recognition, Computer science, Convol...</td>\n",
       "      <td>[0.46327, 0.4434, 0.55688, 0.50135, 0.44515, 0...</td>\n",
       "      <td>[4919037, 1686810756, 1690739335, 1799366690, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           paper_url    arxiv_id  \\\n",
       "0  https://paperswithcode.com/paper/deep-predicti...  1805.07526   \n",
       "\n",
       "                                               title  \\\n",
       "0  deep predictive coding network with local recu...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Inspired by \"predictive coding\" - a theory in ...   \n",
       "\n",
       "                             url_abs                                url_pdf  \\\n",
       "0  http://arxiv.org/abs/1805.07526v2  http://arxiv.org/pdf/1805.07526v2.pdf   \n",
       "\n",
       "        proceeding                                            authors  \\\n",
       "0  NeurIPS 2018 12  [Kuan Han, Haiguang Wen, Yizhen Zhang, Di Fu, ...   \n",
       "\n",
       "                                        tasks        date  ...  doi  \\\n",
       "0  [Image Classification, Object Recognition]  2018-05-19  ...        \n",
       "\n",
       "                                 venue_raw      venue_id venue_type  \\\n",
       "0  Computer Vision and Pattern Recognition  2.597176e+09          J   \n",
       "\n",
       "                                        authors_name  \\\n",
       "0  [Kuan Han, Haiguang Wen, Yizhen Zhang, Di Fu, ...   \n",
       "\n",
       "                                          authors_id authors_org  \\\n",
       "0  [2749467312, 2211170770, 2555490721, 280503237...          []   \n",
       "\n",
       "                                            fos_name  \\\n",
       "0  [Pattern recognition, Computer science, Convol...   \n",
       "\n",
       "                                               fos_w  \\\n",
       "0  [0.46327, 0.4434, 0.55688, 0.50135, 0.44515, 0...   \n",
       "\n",
       "                                                 ref  \n",
       "0  [4919037, 1686810756, 1690739335, 1799366690, ...  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### conver list to DataFrame\n",
    "match_con_df = pd.DataFrame(match_con)\n",
    "print('Matched controlled groups: ', match_con_df.shape)\n",
    "match_con_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save \"match_con_df\"\n",
    "match_con_df.to_csv('../Test_data/src_data/match_con_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
